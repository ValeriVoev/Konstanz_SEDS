---
title: "Applications of Machine Learning for Personalization of Online Content Using Big Data"
author: "Valeri Voev"
output:
  rmdshower::shower_presentation:
    self_contained: false
    katex: true
    ratio: 16x10
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, warning = FALSE, message = FALSE)
library(readr)
library(dplyr)
library(kableExtra)
#library(printr)

read_movielens <- function(fileName, separators) {
	data <- readLines(con <- file(fileName))
	close(con)
	data <- gsub("::", "#", data)
	read_delim(data, delim = "#", col_names = FALSE)
}
```

```{r data_load, include=FALSE, cache=TRUE}
users <- read_movielens("ml-1m/users.dat") %>% 
	setNames(c("UserID", "Gender", "Age", "Occupation", "Zip-code")) 

movies <- read_movielens("ml-1m/movies.dat") %>% 
	setNames(c("MovieID", "Title", "Genres")) 

ratings <- read_movielens("ml-1m/ratings.dat") %>% 
	setNames(c("UserID", "MovieID", "Rating", "Timestamp")) 
```

## Applications of Machine Learning for Personalization of Online Content Using Big Data { .white }

<img src="scifi.jpg" class="cover">

<p class="white">
Valeri Voev <br />
Lead Data Scientist, LEGO Group
</p>


## What is personalization? 

> the action of designing or producing something to meet someone's individual requirements.
**Oxford Online Dictionary**

<div class="double">
<p class="double-flow">
* Google searches
* FB feed
* Netflix recommendations
</p><p class="double-flow">
* Spotify recommendations
* Online shopping
* Your news website
</p>
</div>


## How is personalization achieved


Online content providers (online shops, social network platforms, streaming services) collect massive amounts of information about their user base - not only demographic and personal information, but also any interaction (click, like, post, etc) users have with the platform. 

We will work with an example for personalized movie recommendations based on the [MovieLens](https://grouplens.org/datasets/movielens/1m/) database.


## The aim

We would like to be able to:

* Recommend new movies to existing users
* Recommend movies to new users as soon as they have rated at least one movie

To this end we will use an approach called **collaborative filtering**

* Collaborative - using preferences of many users
* Filtering - to predict (filter out) the items that a given user might like

## Rating table

<table class = "striped">
        <tr>
            <th scope="col"></th>
            <th style="text-align:center">Item 1</th>
            <th style="text-align:center">Item 2</th>
            <th style="text-align:center">Item 3</th>
            <th style="text-align:center">Item 4</th>
        </tr>
        <tr>
            <th scope="row">User 1</th>
            <td align = "center">  </td>
            <td align = "center"> 1 </td>
            <td align = "center">  </td>
            <td align = "center"> 5 </td>
        </tr>
        <tr>
            <th scope="row">User 2</th>
            <td align = "center">  </td>
            <td align = "center">  </td>
            <td align = "center"> 3 </td>
            <td align = "center">  </td>
        </tr>
        <tr>
            <th scope="row">User 3</th>
            <td align = "center">  </td>
            <td align = "center"> 4 </td>
            <td align = "center">  </td>
            <td align = "center">  </td>
        </tr>
        <tr>
            <th scope="row">User 4</th>
            <td align = "center"> 3 </td>
            <td align = "center">  </td>
            <td align = "center">  </td>
            <td align = "center">  </td>
        </tr>
        <tr>
            <th scope="row">User 5</th>
            <td align = "center">  </td>
            <td align = "center">  </td>
            <td align = "center"> 2 </td>
            <td align = "center">  </td>
        </tr>
</table>
... we will try to fill in the gaps

## Problem formulation

We will follow the exposition of "*Large-scale Parallel Collaborative Filtering for the Netflix Prize*" by Zhou et al.

* Denote the rating matrix by \( R \) of dimension \( n_u \times n_i \) where \(n_u\) is the number of users and \(n_i\) is the number of items. 
* An element \( r_{ij}\) represents the rating that user  \( i \) gave to item  \( j \) -- either a real number or missing.

___

We decompose the matrix \( R \) as 

\[ R_{(n_u \times n_i)} = U_{(n_u \times n_f)} V_{(n_f \times n_i)}, \]

where \( n_f \) is the number of latent factors (features). We will call \( U \) the user matrix and \( V \) - the item matrix.

Denote by \( \mathbf{u}_i \subseteq \mathbb{R}^{n_f} \) the \(i\)-th row of \( U \) and \( \mathbf{v}_j \subseteq \mathbb{R}^{n_f} \) the \(j\)-th column of \( V \).

With \( n_f \) large enough (relative to \( n_u \) and \( n_i \)) we can fit \( R \) perfectly, i.e., \( r_{ij} = <\mathbf{u}_i, \mathbf{v}_j>, \forall i, j \).

___

For very large \( n_u \) and \( n_i \), however, that would not be feasible (nor desirable - we will return to that a bit later), so in practice we choose a "small" \( n_f \). For a single rating \(r\) the loss function is defined as

\[\mathcal{l}^2(r, \mathbf{u}, \mathbf{v}) = (r - <\mathbf{u}, \mathbf{v}>)^2\]
The sample loss is then defined as

\[\mathcal{L}(R, U, V) = \frac{1}{n}\sum_{(i,j)\in I}\mathcal{l}^2(r_{ij}, \mathbf{u}_i, \mathbf{v}_j)\]
where \(I\) is the set of the known ratings and \(n = \vert I\vert\).

___

The user and item matrix are then estimated by minimizing the empirical loss function:

\[(\hat{U}, \hat{V}) = {\begin{matrix}\\ {\normalsize \text{argmin}} \\ ^{\normalsize (U,V)}\end{matrix}} \;  \mathcal{L}(R, U, V) \]

<!-- \[(\hat{U}, \hat{V}) = \text{arg} \min_{(U,V)}  \mathcal{L}(R, U, V) \] -->

In total there are \(n_f (n_u + n_i) \) parameters but given the sparsity of \(R\) we have much fewer than \(n_un_i\) data points, so choosing a large \(n_f\) will very likely lead to overfitting.

___

To avoid overfitting we will use (weighted) **L2-regularization** and minimize the following loss function

\[ \mathcal{L}^{reg}_{\lambda}(R, U, V) = \mathcal{L}(R, U, V) +  \lambda \left(\sum_{i}n_{u_i}\vert\vert\mathbf{u}_i \vert\vert^2  + \sum_{j}n_{v_j}\vert\vert\mathbf{v}_j \vert\vert^2\right) \] 
where \(n_{u_i}\) is the number of items that user \(i\) has rated and \(n_{v_j}\) is the number of users that have rated item \(j\).

___

The main contribution of Zhou et al. is the **alternating** least squares (ALS) approach which alternates between updates of the \(U\) and \(V\) matrices (keeping \(V\) and \(U\) fixed, correspondingly). They also show how the problem can be parallelized (Matlab implementation) which allows solving problems of huge dimensions (the Netflix dataset has over 100 million ratings by more than 480,000 users of close to 18,000 movies). The authors estimate a model with \(n_f = 100\) which has a staggering \(n_f \times (n_u + n_i) \approx 50.000.000\) paramaters! 

Since then, the ALS method has been implemented in Spark, which is what we will use.

## Spark

<img src="https://spark.apache.org/images/spark-logo-trademark.png" class="cover" style = "opacity:0.5;">

* Apache Sparkâ„¢ is a unified analytics engine for large-scale data processing
* Has an interface for R - `sparklyr` and Python - `pyspark`
* The MLlib machine learning library has many algorithms fo
	* Classification
	* Regression
	* Association rules
	* Decision trees
	* Clustering, etc

## Movie recommender - Mickey Mouse example

We will use the rating matrix defined above to go through a simple example in order to understand how the ALS method works in practice, how to fit the model, generate recommendations, examine the most important hyperparameters, etc.


## The data

* Users table

```{r}
users 
```
___

* Movies table
```{r}
movies
```

___

* Ratings table
```{r}
ratings
```
Data sparsity is `r scales::percent(1-nrow(ratings)/(nrow(users)*nrow(movies)))`.

## Top 5 movies
```{r}
ratings %>% group_by(MovieID) %>% 
	summarise(MeanRating = mean(Rating), nRatings = n()) %>%
	filter(nRatings >= 10) %>% 
	top_n(5, MeanRating) %>% 
	arrange(desc(MeanRating)) %>% 
	inner_join(movies)
```

## Bottom 5 movies
```{r}
ratings %>% group_by(MovieID) %>% 
	summarise(MeanRating = mean(Rating), nRatings = n()) %>%
	filter(nRatings >= 10) %>% 
	top_n(-5, MeanRating) %>% 
	arrange(MeanRating) %>% 
	inner_join(movies)
```

